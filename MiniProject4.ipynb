{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "513a66f97da34fba967e8993a029a121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dca286c5fd8242aa92f57ceb910b71ce",
              "IPY_MODEL_888f3fc9f38c4edaa0dd693d10643826",
              "IPY_MODEL_fdc4b00089d24d6084cd04addf4f9a4a"
            ],
            "layout": "IPY_MODEL_8c9fae226d464812b1fccb73e5b7f18d"
          }
        },
        "dca286c5fd8242aa92f57ceb910b71ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a4a39827a94fe3a0ab370b4e16bb82",
            "placeholder": "​",
            "style": "IPY_MODEL_f46618eaea6943558e8594cc8418b57b",
            "value": ""
          }
        },
        "888f3fc9f38c4edaa0dd693d10643826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ea4ff91d6dc4b5d96195cbc6a2015ec",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34d4a02770a54b5188b466bd6629a8c0",
            "value": 170498071
          }
        },
        "fdc4b00089d24d6084cd04addf4f9a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a67c65532c42f396c10d02173e12d2",
            "placeholder": "​",
            "style": "IPY_MODEL_03a7534291cb42c1af7db7f6710c4945",
            "value": " 170499072/? [00:04&lt;00:00, 40430501.08it/s]"
          }
        },
        "8c9fae226d464812b1fccb73e5b7f18d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a4a39827a94fe3a0ab370b4e16bb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f46618eaea6943558e8594cc8418b57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ea4ff91d6dc4b5d96195cbc6a2015ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34d4a02770a54b5188b466bd6629a8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32a67c65532c42f396c10d02173e12d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a7534291cb42c1af7db7f6710c4945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MINI PROJECT 4\n",
        "\n"
      ],
      "metadata": {
        "id": "24XwLMMKLPt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm47y3cf8tFC",
        "outputId": "8d7324e5-b678-4e33-e5a9-da52754c5775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOASVLSX7lnW"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #print(out.size())\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class LayerNormalize(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class MLP_Block(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.nn1 = nn.Linear(dim, hidden_dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn1.weight)\n",
        "        torch.nn.init.normal_(self.nn1.bias, std = 1e-6)\n",
        "        self.af1 = nn.GELU()\n",
        "        self.do1 = nn.Dropout(dropout)\n",
        "        self.nn2 = nn.Linear(hidden_dim, dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn2.weight)\n",
        "        torch.nn.init.normal_(self.nn2.bias, std = 1e-6)\n",
        "        self.do2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.nn1(x)\n",
        "        x = self.af1(x)\n",
        "        x = self.do1(x)\n",
        "        x = self.nn2(x)\n",
        "        x = self.do2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5  # 1/sqrt(dim)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias = True) # Wq,Wk,Wv for each vector, thats why *3\n",
        "        torch.nn.init.xavier_uniform_(self.to_qkv.weight)\n",
        "        torch.nn.init.zeros_(self.to_qkv.bias)\n",
        "        \n",
        "        self.nn1 = nn.Linear(dim, dim)\n",
        "        torch.nn.init.xavier_uniform_(self.nn1.weight)\n",
        "        torch.nn.init.zeros_(self.nn1.bias)        \n",
        "        self.do1 = nn.Dropout(dropout)\n",
        "        \n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x) #gets q = Q = Wq matmul x1, k = Wk mm x2, v = Wv mm x3\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv = 3, h = h) # split into multi head attentions\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1) #follow the softmax,q,d,v equation in the paper\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v) #product of v times whatever inside softmax\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)') #concat heads into one matrix, ready for next encoder block\n",
        "        out =  self.nn1(out)\n",
        "        out = self.do1(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(LayerNormalize(dim, Attention(dim, heads = heads, dropout = dropout))),\n",
        "                Residual(LayerNormalize(dim, MLP_Block(dim, mlp_dim, dropout = dropout)))\n",
        "            ]))\n",
        "    def forward(self, x, mask = None):\n",
        "        for attention, mlp in self.layers:\n",
        "            x = attention(x, mask = mask) # go to attention\n",
        "            x = mlp(x) #go to MLP_Block\n",
        "        return x\n",
        "     \n",
        "\n",
        "class ViTResNet(nn.Module):\n",
        "    # initial attributes\n",
        "    #def __init__(self, block, num_blocks, num_classes=10, dim = 128, num_tokens = 8, mlp_dim = 256, heads = 8, depth = 6, emb_dropout = 0.1, dropout= 0.1):\n",
        "    #def __init__(self, block, num_blocks, num_classes=50, dim = 128, num_tokens = 8, mlp_dim = 256, heads = 8, depth = 6, emb_dropout = 0.1, dropout= 0.1):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, dim = 128, num_tokens = 16, mlp_dim = 256, heads = 8, depth = 6, emb_dropout = 0.1, dropout= 0.1):\n",
        "        super(ViTResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.L = num_tokens\n",
        "        self.cT = dim\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2) #8x8 feature maps (64 in total)\n",
        "        self.apply(_weights_init)\n",
        "        \n",
        "        \n",
        "        # Tokenization\n",
        "        self.token_wA = nn.Parameter(torch.empty(BATCH_SIZE_TRAIN,self.L, 64),requires_grad = True) #Tokenization parameters\n",
        "        torch.nn.init.xavier_uniform_(self.token_wA)\n",
        "        self.token_wV = nn.Parameter(torch.empty(BATCH_SIZE_TRAIN,64,self.cT),requires_grad = True) #Tokenization parameters\n",
        "        torch.nn.init.xavier_uniform_(self.token_wV)        \n",
        "             \n",
        "        \n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, (num_tokens + 1), dim))\n",
        "        torch.nn.init.normal_(self.pos_embedding, std = .02) # initialized based on the paper\n",
        "\n",
        "        #self.patch_conv= nn.Conv2d(64,dim, self.patch_size, stride = self.patch_size) \n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim)) #initialized based on the paper\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.nn1 = nn.Linear(dim, num_classes)  # if finetuning, just use a linear layer without further hidden layers (paper)\n",
        "        torch.nn.init.xavier_uniform_(self.nn1.weight)\n",
        "        torch.nn.init.normal_(self.nn1.bias, std = 1e-6)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    \n",
        "        \n",
        "    def forward(self, img, mask = None):\n",
        "        x = F.relu(self.bn1(self.conv1(img)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)  \n",
        "        x = self.layer3(x) \n",
        "        \n",
        "        x = rearrange(x, 'b c h w -> b (h w) c') # 64 vectors each with 64 points. These are the sequences or word vecotrs like in NLP\n",
        "\n",
        "        #Tokenization \n",
        "        wa = rearrange(self.token_wA, 'b h w -> b w h') #Transpose\n",
        "        A= torch.einsum('bij,bjk->bik', x, wa) \n",
        "        A = rearrange(A, 'b h w -> b w h') #Transpose\n",
        "        A = A.softmax(dim=-1)\n",
        "\n",
        "        VV= torch.einsum('bij,bjk->bik', x, self.token_wV)       \n",
        "        T = torch.einsum('bij,bjk->bik', A, VV)  \n",
        "        #print(T.size())\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, T), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x, mask) #main game\n",
        "        x = self.to_cls_token(x[:, 0])       \n",
        "        x = self.nn1(x)\n",
        "        \n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "BATCH_SIZE_TRAIN = 100\n",
        "BATCH_SIZE_TEST = 100\n",
        "\n",
        "DL_PATH = \"C:\\Pytorch\\Spyder\\CIFAR10_data\" # Use your own path\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "transform = torchvision.transforms.Compose(\n",
        "     [torchvision.transforms.RandomHorizontalFlip(),\n",
        "     torchvision.transforms.RandomRotation(10, resample=PIL.Image.BILINEAR),\n",
        "     torchvision.transforms.RandomAffine(8, translate=(.15,.15)),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(DL_PATH, train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(DL_PATH, train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def train(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())\n",
        "            \n",
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "    \n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')\n",
        "    \n",
        "# model = ViT()\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "513a66f97da34fba967e8993a029a121",
            "dca286c5fd8242aa92f57ceb910b71ce",
            "888f3fc9f38c4edaa0dd693d10643826",
            "fdc4b00089d24d6084cd04addf4f9a4a",
            "8c9fae226d464812b1fccb73e5b7f18d",
            "06a4a39827a94fe3a0ab370b4e16bb82",
            "f46618eaea6943558e8594cc8418b57b",
            "1ea4ff91d6dc4b5d96195cbc6a2015ec",
            "34d4a02770a54b5188b466bd6629a8c0",
            "32a67c65532c42f396c10d02173e12d2",
            "03a7534291cb42c1af7db7f6710c4945"
          ]
        },
        "id": "jm0B58u278f7",
        "outputId": "7dc656e4-51a8-460d-bf71-009392d3b0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:1293: UserWarning: The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. Please use 'interpolation' instead.\n",
            "  \"The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to C:\\Pytorch\\Spyder\\CIFAR10_data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "513a66f97da34fba967e8993a029a121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting C:\\Pytorch\\Spyder\\CIFAR10_data/cifar-10-python.tar.gz to C:\\Pytorch\\Spyder\\CIFAR10_data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loader.dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdHv0iBUk7gg",
        "outputId": "7d5a8005-134b-4c6e-9ba3-379d89aff2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[-2.1179, -2.1179, -2.1179,  ...,  0.4166,  0.5878,  0.5878],\n",
            "         [-2.1179, -2.1179, -2.1179,  ...,  0.0912,  0.1254,  0.1597],\n",
            "         [-2.1179, -2.1179, -2.1179,  ...,  0.0741,  0.1254,  0.2796],\n",
            "         ...,\n",
            "         [-2.1179, -2.1179, -2.1179,  ...,  0.2111, -1.1932, -1.3130],\n",
            "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "        [[-2.0357, -2.0357, -2.0357,  ..., -0.0749,  0.1352,  0.1877],\n",
            "         [-2.0357, -2.0357, -2.0357,  ..., -0.4951, -0.4426, -0.3901],\n",
            "         [-2.0357, -2.0357, -2.0357,  ..., -0.4776, -0.4251, -0.2325],\n",
            "         ...,\n",
            "         [-2.0357, -2.0357, -2.0357,  ..., -0.1275, -1.4930, -1.5105],\n",
            "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "        [[-1.8044, -1.8044, -1.8044,  ..., -0.4624, -0.2010, -0.0790],\n",
            "         [-1.8044, -1.8044, -1.8044,  ..., -0.9678, -0.8981, -0.7936],\n",
            "         [-1.8044, -1.8044, -1.8044,  ..., -0.9330, -0.8807, -0.6541],\n",
            "         ...,\n",
            "         [-1.8044, -1.8044, -1.8044,  ..., -0.8284, -1.6476, -1.5081],\n",
            "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results with test and train batch size of 100 each and 3 epochs"
      ],
      "metadata": {
        "id": "Ze3pmQQ5-Fxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 3\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#optimizer = torcpooling-based tokenizerh.optim.SGD(model.parameters(), lr=learning_rate,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzRydtTLLnmm",
        "outputId": "23c1d985-520c-4b8c-d2bf-9e51bdf9cf54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 4.9645\n",
            "[10000/50000 ( 20%)]  Loss: 2.1242\n",
            "[20000/50000 ( 40%)]  Loss: 2.1944\n",
            "[30000/50000 ( 60%)]  Loss: 2.0887\n",
            "[40000/50000 ( 80%)]  Loss: 1.9375\n",
            "Execution time: 593.38 seconds\n",
            "\n",
            "Average test loss: 1.9171  Accuracy: 2231/10000 (22.31%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 1.9012\n",
            "[10000/50000 ( 20%)]  Loss: 1.9202\n",
            "[20000/50000 ( 40%)]  Loss: 1.8837\n",
            "[30000/50000 ( 60%)]  Loss: 1.9000\n",
            "[40000/50000 ( 80%)]  Loss: 2.0789\n",
            "Execution time: 618.46 seconds\n",
            "\n",
            "Average test loss: 1.8000  Accuracy: 2745/10000 (27.45%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 1.8532\n",
            "[10000/50000 ( 20%)]  Loss: 1.8545\n",
            "[20000/50000 ( 40%)]  Loss: 1.6523\n",
            "[30000/50000 ( 60%)]  Loss: 1.7190\n",
            "[40000/50000 ( 80%)]  Loss: 1.7910\n",
            "Execution time: 544.75 seconds\n",
            "\n",
            "Average test loss: 1.6489  Accuracy: 3483/10000 (34.83%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results with test and train batch size of 100 each and 5 epochs "
      ],
      "metadata": {
        "id": "GZP5O0GYMCdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOI1KTZe-d3f",
        "outputId": "10c7c948-67d9-4edb-b9b7-b4a123fe60b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 4.4490\n",
            "[10000/50000 ( 20%)]  Loss: 2.2303\n",
            "[20000/50000 ( 40%)]  Loss: 2.0503\n",
            "[30000/50000 ( 60%)]  Loss: 2.0703\n",
            "[40000/50000 ( 80%)]  Loss: 1.9086\n",
            "Execution time: 543.65 seconds\n",
            "\n",
            "Average test loss: 1.9778  Accuracy: 2197/10000 (21.97%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 2.0133\n",
            "[10000/50000 ( 20%)]  Loss: 1.9967\n",
            "[20000/50000 ( 40%)]  Loss: 1.8432\n",
            "[30000/50000 ( 60%)]  Loss: 1.8573\n",
            "[40000/50000 ( 80%)]  Loss: 1.8626\n",
            "Execution time: 568.23 seconds\n",
            "\n",
            "Average test loss: 1.8378  Accuracy: 2392/10000 (23.92%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 1.8396\n",
            "[10000/50000 ( 20%)]  Loss: 1.6972\n",
            "[20000/50000 ( 40%)]  Loss: 1.8043\n",
            "[30000/50000 ( 60%)]  Loss: 1.5857\n",
            "[40000/50000 ( 80%)]  Loss: 1.6220\n",
            "Execution time: 546.65 seconds\n",
            "\n",
            "Average test loss: 1.6089  Accuracy: 3792/10000 (37.92%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/50000 (  0%)]  Loss: 1.5895\n",
            "[10000/50000 ( 20%)]  Loss: 1.5672\n",
            "[20000/50000 ( 40%)]  Loss: 1.7704\n",
            "[30000/50000 ( 60%)]  Loss: 1.3590\n",
            "[40000/50000 ( 80%)]  Loss: 1.3979\n",
            "Execution time: 518.73 seconds\n",
            "\n",
            "Average test loss: 1.3643  Accuracy: 4923/10000 (49.23%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/50000 (  0%)]  Loss: 1.2872\n",
            "[10000/50000 ( 20%)]  Loss: 1.4268\n",
            "[20000/50000 ( 40%)]  Loss: 1.2071\n",
            "[30000/50000 ( 60%)]  Loss: 1.4028\n",
            "[40000/50000 ( 80%)]  Loss: 1.4791\n",
            "Execution time: 523.29 seconds\n",
            "\n",
            "Average test loss: 1.2939  Accuracy: 5417/10000 (54.17%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results with test and train batch size of 100 each and 64 tokens"
      ],
      "metadata": {
        "id": "rbqN1RtSVf3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token efficiency ablation"
      ],
      "metadata": {
        "id": "rzwKBl5PnVJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCv4Ss7lMr_O",
        "outputId": "f49938ff-3b0f-47af-8ba0-1a07482d08dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 5.2812\n",
            "[10000/50000 ( 20%)]  Loss: 2.1764\n",
            "[20000/50000 ( 40%)]  Loss: 2.0725\n",
            "[30000/50000 ( 60%)]  Loss: 1.9717\n",
            "[40000/50000 ( 80%)]  Loss: 1.9962\n",
            "Execution time: 1161.41 seconds\n",
            "\n",
            "Average test loss: 1.9177  Accuracy: 2279/10000 (22.79%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 1.8657\n",
            "[10000/50000 ( 20%)]  Loss: 1.7969\n",
            "[20000/50000 ( 40%)]  Loss: 2.0755\n",
            "[30000/50000 ( 60%)]  Loss: 2.0084\n",
            "[40000/50000 ( 80%)]  Loss: 1.7074\n",
            "Execution time: 1160.20 seconds\n",
            "\n",
            "Average test loss: 1.7286  Accuracy: 2852/10000 (28.52%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 1.6566\n",
            "[10000/50000 ( 20%)]  Loss: 1.8206\n",
            "[20000/50000 ( 40%)]  Loss: 1.5933\n",
            "[30000/50000 ( 60%)]  Loss: 1.5174\n",
            "[40000/50000 ( 80%)]  Loss: 1.5268\n",
            "Execution time: 1170.30 seconds\n",
            "\n",
            "Average test loss: 1.5955  Accuracy: 3759/10000 (37.59%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/50000 (  0%)]  Loss: 1.5905\n",
            "[10000/50000 ( 20%)]  Loss: 1.3518\n",
            "[20000/50000 ( 40%)]  Loss: 1.3402\n",
            "[30000/50000 ( 60%)]  Loss: 1.3257\n",
            "[40000/50000 ( 80%)]  Loss: 1.4237\n",
            "Execution time: 1175.93 seconds\n",
            "\n",
            "Average test loss: 1.3232  Accuracy: 5167/10000 (51.67%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/50000 (  0%)]  Loss: 1.1620\n",
            "[10000/50000 ( 20%)]  Loss: 1.3116\n",
            "[20000/50000 ( 40%)]  Loss: 1.1391\n",
            "[30000/50000 ( 60%)]  Loss: 1.2631\n",
            "[40000/50000 ( 80%)]  Loss: 1.1694\n",
            "Execution time: 1164.55 seconds\n",
            "\n",
            "Average test loss: 1.3099  Accuracy: 5449/10000 (54.49%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results with test and train batch size of 100 each and 32 tokens"
      ],
      "metadata": {
        "id": "TJYr6uTjCwWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPqyNVNxCxao",
        "outputId": "b4f22707-20e8-4687-991e-7cb05ecf8729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 4.7749\n",
            "[10000/50000 ( 20%)]  Loss: 2.3933\n",
            "[20000/50000 ( 40%)]  Loss: 2.0594\n",
            "[30000/50000 ( 60%)]  Loss: 2.1453\n",
            "[40000/50000 ( 80%)]  Loss: 1.8849\n",
            "Execution time: 844.61 seconds\n",
            "\n",
            "Average test loss: 1.9624  Accuracy: 2282/10000 (22.82%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 1.9395\n",
            "[10000/50000 ( 20%)]  Loss: 2.0948\n",
            "[20000/50000 ( 40%)]  Loss: 1.8205\n",
            "[30000/50000 ( 60%)]  Loss: 1.8250\n",
            "[40000/50000 ( 80%)]  Loss: 1.9110\n",
            "Execution time: 839.99 seconds\n",
            "\n",
            "Average test loss: 1.7757  Accuracy: 2833/10000 (28.33%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 1.7740\n",
            "[10000/50000 ( 20%)]  Loss: 1.7232\n",
            "[20000/50000 ( 40%)]  Loss: 1.6808\n",
            "[30000/50000 ( 60%)]  Loss: 1.6697\n",
            "[40000/50000 ( 80%)]  Loss: 1.6753\n",
            "Execution time: 849.35 seconds\n",
            "\n",
            "Average test loss: 1.5853  Accuracy: 3839/10000 (38.39%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/50000 (  0%)]  Loss: 1.4269\n",
            "[10000/50000 ( 20%)]  Loss: 1.5377\n",
            "[20000/50000 ( 40%)]  Loss: 1.4907\n",
            "[30000/50000 ( 60%)]  Loss: 1.6583\n",
            "[40000/50000 ( 80%)]  Loss: 1.4130\n",
            "Execution time: 856.90 seconds\n",
            "\n",
            "Average test loss: 1.4930  Accuracy: 4544/10000 (45.44%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/50000 (  0%)]  Loss: 1.2244\n",
            "[10000/50000 ( 20%)]  Loss: 1.2851\n",
            "[20000/50000 ( 40%)]  Loss: 1.5084\n",
            "[30000/50000 ( 60%)]  Loss: 1.4246\n",
            "[40000/50000 ( 80%)]  Loss: 1.3021\n",
            "Execution time: 857.22 seconds\n",
            "\n",
            "Average test loss: 1.3826  Accuracy: 5161/10000 (51.61%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results with test and train batch size of 100 each and 16 tokens"
      ],
      "metadata": {
        "id": "klFjN5ZxDUGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUk24-z-DY-o",
        "outputId": "0b3ea82a-510f-4266-994e-d1fc899447cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 5.3715\n",
            "[10000/50000 ( 20%)]  Loss: 2.4521\n",
            "[20000/50000 ( 40%)]  Loss: 2.0508\n",
            "[30000/50000 ( 60%)]  Loss: 2.0372\n",
            "[40000/50000 ( 80%)]  Loss: 2.0522\n",
            "Execution time: 627.75 seconds\n",
            "\n",
            "Average test loss: 1.9484  Accuracy: 2220/10000 (22.20%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 2.0241\n",
            "[10000/50000 ( 20%)]  Loss: 2.0641\n",
            "[20000/50000 ( 40%)]  Loss: 1.9733\n",
            "[30000/50000 ( 60%)]  Loss: 1.8900\n",
            "[40000/50000 ( 80%)]  Loss: 1.7596\n",
            "Execution time: 633.89 seconds\n",
            "\n",
            "Average test loss: 1.7614  Accuracy: 2954/10000 (29.54%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 1.8310\n",
            "[10000/50000 ( 20%)]  Loss: 1.8038\n",
            "[20000/50000 ( 40%)]  Loss: 1.5984\n",
            "[30000/50000 ( 60%)]  Loss: 1.7181\n",
            "[40000/50000 ( 80%)]  Loss: 1.5240\n",
            "Execution time: 637.28 seconds\n",
            "\n",
            "Average test loss: 1.6007  Accuracy: 3757/10000 (37.57%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/50000 (  0%)]  Loss: 1.7504\n",
            "[10000/50000 ( 20%)]  Loss: 1.4397\n",
            "[20000/50000 ( 40%)]  Loss: 1.4004\n",
            "[30000/50000 ( 60%)]  Loss: 1.4037\n",
            "[40000/50000 ( 80%)]  Loss: 1.4426\n",
            "Execution time: 631.74 seconds\n",
            "\n",
            "Average test loss: 1.5612  Accuracy: 4496/10000 (44.96%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/50000 (  0%)]  Loss: 1.3769\n",
            "[10000/50000 ( 20%)]  Loss: 1.3949\n",
            "[20000/50000 ( 40%)]  Loss: 1.2566\n",
            "[30000/50000 ( 60%)]  Loss: 1.1838\n",
            "[40000/50000 ( 80%)]  Loss: 1.1659\n",
            "Execution time: 635.17 seconds\n",
            "\n",
            "Average test loss: 1.4095  Accuracy: 5145/10000 (51.45%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results with test and batch size of 100 each and 20 classes"
      ],
      "metadata": {
        "id": "ufn6efBA6Bcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f3z_QH3zm16",
        "outputId": "1c50fb83-df82-494f-a74f-06f4e7c8f917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 6.6500\n",
            "[10000/50000 ( 20%)]  Loss: 2.5404\n",
            "[20000/50000 ( 40%)]  Loss: 2.0259\n",
            "[30000/50000 ( 60%)]  Loss: 2.0722\n",
            "[40000/50000 ( 80%)]  Loss: 1.8669\n",
            "Execution time: 571.51 seconds\n",
            "\n",
            "Average test loss: 2.0283  Accuracy: 2002/10000 (20.02%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 2.0398\n",
            "[10000/50000 ( 20%)]  Loss: 1.8725\n",
            "[20000/50000 ( 40%)]  Loss: 1.7880\n",
            "[30000/50000 ( 60%)]  Loss: 1.9767\n",
            "[40000/50000 ( 80%)]  Loss: 1.7818\n",
            "Execution time: 509.81 seconds\n",
            "\n",
            "Average test loss: 1.8256  Accuracy: 2649/10000 (26.49%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 1.8635\n",
            "[10000/50000 ( 20%)]  Loss: 1.8875\n",
            "[20000/50000 ( 40%)]  Loss: 1.7562\n",
            "[30000/50000 ( 60%)]  Loss: 1.5552\n",
            "[40000/50000 ( 80%)]  Loss: 1.6209\n",
            "Execution time: 509.74 seconds\n",
            "\n",
            "Average test loss: 1.5594  Accuracy: 3839/10000 (38.39%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/50000 (  0%)]  Loss: 1.5836\n",
            "[10000/50000 ( 20%)]  Loss: 1.5187\n",
            "[20000/50000 ( 40%)]  Loss: 1.4796\n",
            "[30000/50000 ( 60%)]  Loss: 1.4610\n",
            "[40000/50000 ( 80%)]  Loss: 1.4531\n",
            "Execution time: 503.05 seconds\n",
            "\n",
            "Average test loss: 1.4430  Accuracy: 4634/10000 (46.34%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/50000 (  0%)]  Loss: 1.3998\n",
            "[10000/50000 ( 20%)]  Loss: 1.4406\n",
            "[20000/50000 ( 40%)]  Loss: 1.3124\n",
            "[30000/50000 ( 60%)]  Loss: 1.0691\n",
            "[40000/50000 ( 80%)]  Loss: 1.1499\n",
            "Execution time: 654.09 seconds\n",
            "\n",
            "Average test loss: 1.2646  Accuracy: 5609/10000 (56.09%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results with test and batch size of 100 each and 50 classes"
      ],
      "metadata": {
        "id": "sLvefRFZ-oMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeH4oU97-sxo",
        "outputId": "2c0e1a57-12cb-4487-acb3-2e4b8805f6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 6.6056\n",
            "[10000/50000 ( 20%)]  Loss: 2.2252\n",
            "[20000/50000 ( 40%)]  Loss: 2.2852\n",
            "[30000/50000 ( 60%)]  Loss: 2.0074\n",
            "[40000/50000 ( 80%)]  Loss: 1.9440\n",
            "Execution time: 584.48 seconds\n",
            "\n",
            "Average test loss: 1.9692  Accuracy: 2253/10000 (22.53%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 1.7360\n",
            "[10000/50000 ( 20%)]  Loss: 1.8807\n",
            "[20000/50000 ( 40%)]  Loss: 1.8071\n",
            "[30000/50000 ( 60%)]  Loss: 1.6638\n",
            "[40000/50000 ( 80%)]  Loss: 1.6488\n",
            "Execution time: 564.22 seconds\n",
            "\n",
            "Average test loss: 1.9300  Accuracy: 2597/10000 (25.97%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 1.7089\n",
            "[10000/50000 ( 20%)]  Loss: 1.7690\n",
            "[20000/50000 ( 40%)]  Loss: 1.4981\n",
            "[30000/50000 ( 60%)]  Loss: 1.5686\n",
            "[40000/50000 ( 80%)]  Loss: 1.7220\n",
            "Execution time: 533.62 seconds\n",
            "\n",
            "Average test loss: 1.6460  Accuracy: 3632/10000 (36.32%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/50000 (  0%)]  Loss: 1.4692\n",
            "[10000/50000 ( 20%)]  Loss: 1.5088\n",
            "[20000/50000 ( 40%)]  Loss: 1.4879\n",
            "[30000/50000 ( 60%)]  Loss: 1.4296\n",
            "[40000/50000 ( 80%)]  Loss: 1.5127\n",
            "Execution time: 528.12 seconds\n",
            "\n",
            "Average test loss: 1.4219  Accuracy: 4494/10000 (44.94%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/50000 (  0%)]  Loss: 1.2977\n",
            "[10000/50000 ( 20%)]  Loss: 1.3836\n",
            "[20000/50000 ( 40%)]  Loss: 1.3797\n",
            "[30000/50000 ( 60%)]  Loss: 1.1381\n",
            "[40000/50000 ( 80%)]  Loss: 1.0913\n",
            "Execution time: 511.63 seconds\n",
            "\n",
            "Average test loss: 1.3865  Accuracy: 4955/10000 (49.55%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results with test and train batch size of 100 each and momentum of 0.9 with weight decay of 1e-4"
      ],
      "metadata": {
        "id": "2iFVI03GR2wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "model = ViTResNet(BasicBlock, [3, 3, 3])\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001,momentum=.9,weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[35,48],gamma = 0.1)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \".\\ViTRes.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qusQLqzeKNGt",
        "outputId": "2f4ef898-05cc-4f6c-ecb6-fea492d741ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/50000 (  0%)]  Loss: 4.8116\n",
            "[10000/50000 ( 20%)]  Loss: 2.6631\n",
            "[20000/50000 ( 40%)]  Loss: 2.5303\n",
            "[30000/50000 ( 60%)]  Loss: 2.6920\n",
            "[40000/50000 ( 80%)]  Loss: 2.2189\n",
            "Execution time: 585.30 seconds\n",
            "\n",
            "Average test loss: 2.0975  Accuracy: 1970/10000 (19.70%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/50000 (  0%)]  Loss: 2.1033\n",
            "[10000/50000 ( 20%)]  Loss: 2.2940\n",
            "[20000/50000 ( 40%)]  Loss: 2.2095\n",
            "[30000/50000 ( 60%)]  Loss: 2.1845\n",
            "[40000/50000 ( 80%)]  Loss: 1.9183\n",
            "Execution time: 583.12 seconds\n",
            "\n",
            "Average test loss: 1.9872  Accuracy: 2001/10000 (20.01%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/50000 (  0%)]  Loss: 2.1424\n",
            "[10000/50000 ( 20%)]  Loss: 1.9771\n",
            "[20000/50000 ( 40%)]  Loss: 1.9672\n",
            "[30000/50000 ( 60%)]  Loss: 1.9993\n",
            "[40000/50000 ( 80%)]  Loss: 2.0418\n",
            "Execution time: 576.41 seconds\n",
            "\n",
            "Average test loss: 1.9416  Accuracy: 2188/10000 (21.88%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/50000 (  0%)]  Loss: 1.9430\n",
            "[10000/50000 ( 20%)]  Loss: 1.9488\n",
            "[20000/50000 ( 40%)]  Loss: 1.8881\n",
            "[30000/50000 ( 60%)]  Loss: 2.0644\n",
            "[40000/50000 ( 80%)]  Loss: 1.9232\n",
            "Execution time: 570.81 seconds\n",
            "\n",
            "Average test loss: 1.8914  Accuracy: 2347/10000 (23.47%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/50000 (  0%)]  Loss: 1.8864\n",
            "[10000/50000 ( 20%)]  Loss: 1.9771\n",
            "[20000/50000 ( 40%)]  Loss: 1.9375\n",
            "[30000/50000 ( 60%)]  Loss: 1.9018\n",
            "[40000/50000 ( 80%)]  Loss: 1.8741\n",
            "Execution time: 595.73 seconds\n",
            "\n",
            "Average test loss: 1.8840  Accuracy: 2271/10000 (22.71%)\n",
            "\n",
            "Execution time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU and memory specs"
      ],
      "metadata": {
        "id": "15W3qwzP0Mbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io40vkQJ0Ea7",
        "outputId": "01b78d8d-2d23-4740-c023-25db615afe8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   39G   70G  36% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  817M  59% /sbin/docker-init\n",
            "tmpfs           6.4G   32K  6.4G   1% /var/colab\n",
            "/dev/sda1        81G   43G   39G  53% /etc/hosts\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoaAProh0GHj",
        "outputId": "20dcc899-8898-4fa1-e389-f01c9dd22730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.162\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.32\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.162\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.32\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbUaUhYw0JjK",
        "outputId": "06b30743-7fd8-4714-d944-64042c8ad325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13302920 kB\n",
            "MemFree:        10713556 kB\n",
            "MemAvailable:   12380396 kB\n",
            "Buffers:          105612 kB\n",
            "Cached:          1683840 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1081128 kB\n",
            "Inactive:        1325924 kB\n",
            "Active(anon):     540068 kB\n",
            "Inactive(anon):      448 kB\n",
            "Active(file):     541060 kB\n",
            "Inactive(file):  1325476 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               768 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        617660 kB\n",
            "Mapped:           228764 kB\n",
            "Shmem:              1176 kB\n",
            "KReclaimable:      83560 kB\n",
            "Slab:             124528 kB\n",
            "SReclaimable:      83560 kB\n",
            "SUnreclaim:        40968 kB\n",
            "KernelStack:        4752 kB\n",
            "PageTables:         9268 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6651460 kB\n",
            "Committed_AS:    3065924 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:        7104 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1448 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       99136 kB\n",
            "DirectMap2M:     5140480 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ]
        }
      ]
    }
  ]
}